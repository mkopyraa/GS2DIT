{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "28931e1e-92f4-4487-c654-7b906b07cd35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort (from gpt-2-simple)\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.32.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow>=2.5.1->gpt-2-simple) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24559 sha256=e52e82b4c382bf70f5bd7587a511b86940fbbcd4d3a8fffafc2a24a444d02496\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/6a/fe/10d3223f78d1ac3e4c83bb4c5e2d28dfb1789c2fb4cc7ea8d0\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "ed46cc8c-b916-487d-f0d1-a037940c3f2c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 340Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:01, 569kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 944Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:54, 9.21Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 720Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 842kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 858kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "5424ebbf-5166-44e5-c3bf-01bb050e9317"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "324e5bef-53d6-4c20-ddb0-3d0279027f92"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth?\n",
            "\n",
            "In the year 2000, there were 35 big, rigid planets in the Solar System, all of which are probably rocky, and three of them are likely to be habitable. The other two are probably rocky. The third, with a velocity of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "51f9ff97-287d-4c94-ab61-7c27b14fb7d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:03--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.95.189, 54.231.169.144, 52.217.50.6, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.95.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txtâ€™\n",
            "\n",
            "nietzsche.txt       100%[===================>] 586.82K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-03-21 15:19:03 (4.21 MB/s) - â€˜nietzsche.txtâ€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "12114719-2216-4808-b603-4b4d5ac81f69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "b59755af-4d8a-4592-c71a-bee603432b35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:19:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-21 15:19:13 (19.9 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'got1.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "a24366ad-3e95-426a-a474-8fdcf46451b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 433157 tokens\n",
            "Training...\n",
            "[1 | 6.63] loss=3.49 avg=3.49\n",
            "[2 | 8.74] loss=3.22 avg=3.35\n",
            "[3 | 10.85] loss=3.39 avg=3.36\n",
            "[4 | 12.96] loss=3.28 avg=3.34\n",
            "[5 | 15.07] loss=3.20 avg=3.31\n",
            "[6 | 17.19] loss=3.15 avg=3.29\n",
            "[7 | 19.32] loss=3.21 avg=3.27\n",
            "[8 | 21.45] loss=3.37 avg=3.29\n",
            "[9 | 23.58] loss=3.10 avg=3.27\n",
            "[10 | 25.72] loss=3.34 avg=3.27\n",
            "[11 | 27.85] loss=3.20 avg=3.27\n",
            "[12 | 30.00] loss=3.22 avg=3.26\n",
            "[13 | 32.15] loss=2.98 avg=3.24\n",
            "[14 | 34.30] loss=3.21 avg=3.24\n",
            "[15 | 36.46] loss=3.22 avg=3.24\n",
            "[16 | 38.62] loss=3.05 avg=3.22\n",
            "[17 | 40.78] loss=3.11 avg=3.22\n",
            "[18 | 42.96] loss=3.10 avg=3.21\n",
            "[19 | 45.14] loss=3.14 avg=3.20\n",
            "[20 | 47.32] loss=3.22 avg=3.20\n",
            "[21 | 49.51] loss=3.09 avg=3.20\n",
            "[22 | 51.70] loss=3.12 avg=3.20\n",
            "[23 | 53.89] loss=3.04 avg=3.19\n",
            "[24 | 56.09] loss=3.13 avg=3.18\n",
            "[25 | 58.29] loss=3.02 avg=3.18\n",
            "[26 | 60.50] loss=3.12 avg=3.17\n",
            "[27 | 62.70] loss=2.98 avg=3.17\n",
            "[28 | 64.92] loss=3.09 avg=3.16\n",
            "[29 | 67.14] loss=3.01 avg=3.16\n",
            "[30 | 69.37] loss=3.14 avg=3.16\n",
            "[31 | 71.60] loss=3.11 avg=3.15\n",
            "[32 | 73.84] loss=3.04 avg=3.15\n",
            "[33 | 76.07] loss=3.07 avg=3.15\n",
            "[34 | 78.32] loss=2.87 avg=3.14\n",
            "[35 | 80.57] loss=3.01 avg=3.13\n",
            "[36 | 82.82] loss=2.96 avg=3.13\n",
            "[37 | 85.06] loss=3.04 avg=3.12\n",
            "[38 | 87.31] loss=3.08 avg=3.12\n",
            "[39 | 89.56] loss=2.96 avg=3.12\n",
            "[40 | 91.82] loss=3.01 avg=3.12\n",
            "[41 | 94.06] loss=3.01 avg=3.11\n",
            "[42 | 96.30] loss=2.98 avg=3.11\n",
            "[43 | 98.53] loss=3.07 avg=3.11\n",
            "[44 | 100.77] loss=2.98 avg=3.10\n",
            "[45 | 103.00] loss=2.95 avg=3.10\n",
            "[46 | 105.23] loss=2.91 avg=3.09\n",
            "[47 | 107.46] loss=3.06 avg=3.09\n",
            "[48 | 109.68] loss=2.99 avg=3.09\n",
            "[49 | 111.91] loss=2.98 avg=3.09\n",
            "[50 | 114.13] loss=3.00 avg=3.09\n",
            "[51 | 116.35] loss=3.01 avg=3.08\n",
            "[52 | 118.56] loss=3.01 avg=3.08\n",
            "[53 | 120.78] loss=3.00 avg=3.08\n",
            "[54 | 122.99] loss=3.10 avg=3.08\n",
            "[55 | 125.20] loss=3.09 avg=3.08\n",
            "[56 | 127.41] loss=3.06 avg=3.08\n",
            "[57 | 129.62] loss=2.96 avg=3.08\n",
            "[58 | 131.83] loss=2.88 avg=3.07\n",
            "[59 | 134.04] loss=3.03 avg=3.07\n",
            "[60 | 136.25] loss=2.86 avg=3.07\n",
            "[61 | 138.47] loss=2.99 avg=3.07\n",
            "[62 | 140.69] loss=2.96 avg=3.06\n",
            "[63 | 142.90] loss=2.80 avg=3.06\n",
            "[64 | 145.11] loss=2.97 avg=3.06\n",
            "[65 | 147.33] loss=2.85 avg=3.05\n",
            "[66 | 149.54] loss=2.96 avg=3.05\n",
            "[67 | 151.76] loss=2.78 avg=3.04\n",
            "[68 | 153.99] loss=2.89 avg=3.04\n",
            "[69 | 156.21] loss=2.88 avg=3.04\n",
            "[70 | 158.44] loss=2.87 avg=3.03\n",
            "[71 | 160.67] loss=2.91 avg=3.03\n",
            "[72 | 162.89] loss=2.92 avg=3.03\n",
            "[73 | 165.12] loss=2.97 avg=3.03\n",
            "[74 | 167.35] loss=2.75 avg=3.02\n",
            "[75 | 169.58] loss=2.91 avg=3.02\n",
            "[76 | 171.81] loss=2.76 avg=3.02\n",
            "[77 | 174.04] loss=2.88 avg=3.01\n",
            "[78 | 176.27] loss=2.96 avg=3.01\n",
            "[79 | 178.50] loss=2.88 avg=3.01\n",
            "[80 | 180.73] loss=2.93 avg=3.01\n",
            "[81 | 182.96] loss=2.85 avg=3.01\n",
            "[82 | 185.18] loss=2.88 avg=3.00\n",
            "[83 | 187.40] loss=2.89 avg=3.00\n",
            "[84 | 189.63] loss=3.00 avg=3.00\n",
            "[85 | 191.85] loss=3.03 avg=3.00\n",
            "[86 | 194.08] loss=2.93 avg=3.00\n",
            "[87 | 196.30] loss=2.96 avg=3.00\n",
            "[88 | 198.52] loss=2.99 avg=3.00\n",
            "[89 | 200.75] loss=2.86 avg=3.00\n",
            "[90 | 202.97] loss=2.91 avg=3.00\n",
            "[91 | 205.19] loss=2.78 avg=2.99\n",
            "[92 | 207.40] loss=2.91 avg=2.99\n",
            "[93 | 209.62] loss=2.62 avg=2.99\n",
            "[94 | 211.84] loss=2.82 avg=2.98\n",
            "[95 | 214.07] loss=2.85 avg=2.98\n",
            "[96 | 216.28] loss=2.65 avg=2.97\n",
            "[97 | 218.51] loss=2.71 avg=2.97\n",
            "[98 | 220.73] loss=2.83 avg=2.97\n",
            "[99 | 222.95] loss=2.87 avg=2.97\n",
            "[100 | 225.17] loss=2.81 avg=2.96\n",
            "======== SAMPLE 1 ========\n",
            " C. \"As a boy, you were an instant.\" \n",
            "\"You were a long sword and a good rider,\" Myrcella said. \"It is not like you to get lost in the woods. \n",
            "You have time to train, but I want you to ride with me.\" \n",
            "Joff went to her. \"Have it now,\" she commanded. Her son was seated behind her. \n",
            "She had no desire to hear them speak, but then Myrcella looked at her. \"Why aren't you \n",
            "walking?\" \n",
            "\"What are you doing?\" Tyrion said. \n",
            "They stopped, for a moment. \"I need to get back to Blackwater,\" she replied. \n",
            "Page 204\n",
            "\n",
            "\"I'm going to kill him,\" Jaime said. \n",
            "\"No, no, you'll be safe in Arryx,\" Joffrey said. \n",
            "He stood up. \"We will go down there, to the river, and there kill the bastard's son and \n",
            "himself.\" He looked at the corpses. \"That is what I did when I fell for him. I \n",
            "danced against him.\" \n",
            "Joffrey smiled. \"The boy who fell on me . . . he's just a little thing, and \n",
            "nothing I can do about it.\" Tyrion lifted his arm and lifted a stickknife. \"I'll have to get \n",
            "a knife.\" After a pause, Joffrey rose, put four fingers in the stickknife's hilt, and plunged a long hard blow at Jon \n",
            "BenÃ©t's face with an iron. \"Jon Benet, he might have gotten the girl if it meant anything to me,\" he said, \n",
            "while he pushed Jon off with his bare hands. \"You might do the same.\" \n",
            "\"We could do that, I don't know,\" Ser Rodrik agreed. \"Or I'd never think of it.\" \n",
            "Joffrey looked at him again. \"I do the same,\" he said. \"But what if I kill these little hags.\" \n",
            "\"You might not want to listen to me, my lord,\" Jaime said, \"but when you take it for \n",
            "the Lord's, then yes . . . of course.\" \n",
            "Jon Marron's face softened. \"Not my father, not the Lord of Riverrun, not this young lad, not the \n",
            "lover of the Vale.\" \n",
            "Joffrey's fingers were digging into his own left cheek. \n",
            "\"What did Ser Rodrik tell you?\" Jaime asked him. \"What do you want from him?\" \n",
            "\"The Hand,\" Jaime said, \"I'll tell you what I want.\" \n",
            "The black-eyed knight looked at Ser Rodrik with a blank face. \"I'd much rather Jon Benet die than this little \n",
            "young one.\" \n",
            "\"Tell him,\" Jaime said. \"Make me promise that all the rest of the ladies follow my advice, and \n",
            "that he will keep the Neck.\" \n",
            "That sounded like no one, no one. Ser Marron, too, to whom the words came at the end, but \n",
            "Joffrey ignored Ser Rodrik. That was what he said to him. \n",
            "Page 205\n",
            "\n",
            "\"A man can't always follow his own counsel,\" Jaime told him. \n",
            "\"That is the truth,\" Joffrey agreed. He had Jon Benett's Neck, he said. \"I'm \n",
            "not one to tell lies.\" He took the stickknife as though it had no matter. \"What is it \n",
            "\"Jon Benet was a trickster, \" Joffrey said. \"Joffrey the brother of Jon.\" His voice was so hard he could not \n",
            "speak. \n",
            "\"It was Jon Benet and Ser Marron's Hand,\" Jaime told him. \"Here there is no truth, no one to follow, \n",
            "no one who can lie for you is a liar.\" \n",
            "Joffrey looked at the stickknife in his hand. When its blade touched the stone of a wooden \n",
            "hollow in the deep blue waters, it plunged into JonBenet's chest from behind as thick as steel. \"It \n",
            "had to be a man,\" Joffrey said. \"The boy,\" he said, \"and you?\" \n",
            "\"Not Jon Benet,\" Jaime said; he just looked at a stickknife, and Ser Rodrik smiled. \"If you could bring \n",
            "him down in the river.\" \n",
            "\"What if Robert was the Hand?\" Jon Benet could not have been the boy, Jaime realized. He was Jon Benet. They had grown \n",
            "up. Joffrey had grown up, and Jon Benet would grow up with him, and Robb could not have been the \n",
            "son of Ned and Winterfell at all. Yet that was not the case. JonBenet Ben\n",
            "\n",
            "[101 | 239.59] loss=2.80 avg=2.96\n",
            "[102 | 241.81] loss=2.92 avg=2.96\n",
            "[103 | 244.03] loss=2.78 avg=2.96\n",
            "[104 | 246.25] loss=2.72 avg=2.95\n",
            "[105 | 248.48] loss=2.79 avg=2.95\n",
            "[106 | 250.71] loss=2.84 avg=2.95\n",
            "[107 | 252.93] loss=2.83 avg=2.95\n",
            "[108 | 255.16] loss=2.75 avg=2.95\n",
            "[109 | 257.39] loss=2.74 avg=2.94\n",
            "[110 | 259.61] loss=2.77 avg=2.94\n",
            "[111 | 261.84] loss=2.71 avg=2.94\n",
            "[112 | 264.07] loss=2.47 avg=2.93\n",
            "[113 | 266.30] loss=2.83 avg=2.93\n",
            "[114 | 268.52] loss=2.91 avg=2.93\n",
            "[115 | 270.74] loss=2.81 avg=2.93\n",
            "[116 | 272.98] loss=2.65 avg=2.92\n",
            "[117 | 275.21] loss=2.75 avg=2.92\n",
            "[118 | 277.43] loss=2.73 avg=2.92\n",
            "[119 | 279.66] loss=2.87 avg=2.92\n",
            "[120 | 281.88] loss=2.85 avg=2.92\n",
            "[121 | 284.10] loss=2.89 avg=2.91\n",
            "[122 | 286.33] loss=2.73 avg=2.91\n",
            "[123 | 288.56] loss=2.59 avg=2.91\n",
            "[124 | 290.78] loss=2.93 avg=2.91\n",
            "[125 | 293.00] loss=2.71 avg=2.91\n",
            "[126 | 295.22] loss=2.81 avg=2.90\n",
            "[127 | 297.44] loss=2.77 avg=2.90\n",
            "[128 | 299.68] loss=2.60 avg=2.90\n",
            "[129 | 301.89] loss=2.76 avg=2.90\n",
            "[130 | 304.11] loss=2.66 avg=2.89\n",
            "[131 | 306.33] loss=2.83 avg=2.89\n",
            "[132 | 308.56] loss=2.75 avg=2.89\n",
            "[133 | 310.78] loss=2.57 avg=2.89\n",
            "[134 | 313.01] loss=2.89 avg=2.89\n",
            "[135 | 315.24] loss=2.68 avg=2.88\n",
            "[136 | 317.46] loss=2.81 avg=2.88\n",
            "[137 | 319.68] loss=2.63 avg=2.88\n",
            "[138 | 321.90] loss=2.86 avg=2.88\n",
            "[139 | 324.13] loss=2.68 avg=2.88\n",
            "[140 | 326.35] loss=2.60 avg=2.87\n",
            "[141 | 328.57] loss=2.86 avg=2.87\n",
            "[142 | 330.79] loss=2.66 avg=2.87\n",
            "[143 | 333.01] loss=2.57 avg=2.86\n",
            "[144 | 335.24] loss=2.65 avg=2.86\n",
            "[145 | 337.46] loss=2.78 avg=2.86\n",
            "[146 | 339.69] loss=2.63 avg=2.86\n",
            "[147 | 341.91] loss=2.61 avg=2.85\n",
            "[148 | 344.13] loss=2.58 avg=2.85\n",
            "[149 | 346.35] loss=2.60 avg=2.85\n",
            "[150 | 348.57] loss=2.82 avg=2.85\n",
            "[151 | 350.79] loss=2.69 avg=2.85\n",
            "[152 | 353.01] loss=2.83 avg=2.85\n",
            "[153 | 355.23] loss=2.59 avg=2.84\n",
            "[154 | 357.45] loss=2.69 avg=2.84\n",
            "[155 | 359.68] loss=2.64 avg=2.84\n",
            "[156 | 361.90] loss=2.65 avg=2.84\n",
            "[157 | 364.12] loss=2.72 avg=2.83\n",
            "[158 | 366.34] loss=2.62 avg=2.83\n",
            "[159 | 368.56] loss=2.63 avg=2.83\n",
            "[160 | 370.78] loss=2.61 avg=2.83\n",
            "[161 | 373.01] loss=2.61 avg=2.82\n",
            "[162 | 375.23] loss=2.71 avg=2.82\n",
            "[163 | 377.45] loss=2.58 avg=2.82\n",
            "[164 | 379.68] loss=2.88 avg=2.82\n",
            "[165 | 381.90] loss=2.57 avg=2.82\n",
            "[166 | 384.12] loss=2.69 avg=2.81\n",
            "[167 | 386.35] loss=2.73 avg=2.81\n",
            "[168 | 388.57] loss=2.74 avg=2.81\n",
            "[169 | 390.78] loss=2.70 avg=2.81\n",
            "[170 | 393.01] loss=2.62 avg=2.81\n",
            "[171 | 395.23] loss=2.89 avg=2.81\n",
            "[172 | 397.45] loss=2.68 avg=2.81\n",
            "[173 | 399.67] loss=2.48 avg=2.80\n",
            "[174 | 401.89] loss=2.70 avg=2.80\n",
            "[175 | 404.11] loss=2.86 avg=2.80\n",
            "[176 | 406.33] loss=2.63 avg=2.80\n",
            "[177 | 408.55] loss=2.22 avg=2.80\n",
            "[178 | 410.78] loss=2.61 avg=2.79\n",
            "[179 | 413.00] loss=2.67 avg=2.79\n",
            "[180 | 415.22] loss=2.60 avg=2.79\n",
            "[181 | 417.44] loss=2.55 avg=2.79\n",
            "[182 | 419.66] loss=2.54 avg=2.78\n",
            "[183 | 421.89] loss=2.53 avg=2.78\n",
            "[184 | 424.11] loss=2.56 avg=2.78\n",
            "[185 | 426.33] loss=2.42 avg=2.77\n",
            "[186 | 428.55] loss=2.54 avg=2.77\n",
            "[187 | 430.77] loss=2.60 avg=2.77\n",
            "[188 | 433.01] loss=2.53 avg=2.77\n",
            "[189 | 435.23] loss=2.70 avg=2.77\n",
            "[190 | 437.44] loss=2.60 avg=2.76\n",
            "[191 | 439.66] loss=2.57 avg=2.76\n",
            "[192 | 441.89] loss=2.67 avg=2.76\n",
            "[193 | 444.11] loss=2.61 avg=2.76\n",
            "[194 | 446.34] loss=2.29 avg=2.75\n",
            "[195 | 448.57] loss=2.70 avg=2.75\n",
            "[196 | 450.79] loss=2.70 avg=2.75\n",
            "[197 | 453.01] loss=2.63 avg=2.75\n",
            "[198 | 455.23] loss=2.54 avg=2.75\n",
            "[199 | 457.46] loss=2.57 avg=2.75\n",
            "[200 | 459.69] loss=2.55 avg=2.74\n",
            "======== SAMPLE 1 ========\n",
            " Do\n",
            "\n",
            "with the gods. There are no gods, no gods.\" \n",
            "Arya looked at her son; his eyes were hard and wide and hard in his cold grey eyes. Tears \n",
            "filled his eyes and haunted his hair. The words haunted their lips. Tears. They were \n",
            "blood in the dark and no more than that would cut their meaning. She had a black eye. She had a black hair. \n",
            "Jorah had taken her into silence. \"My boy,\" he confessed. They had ridden together at his feast, and \n",
            "he had never been alone in all his mourning. \"My black hair is a badge of honor,\" he said. \"Even my sons have it. \n",
            "They wear it, my daughter. This must not be. I should not be giving honor. This must not be.\" \n",
            "\"You should be giving honor, my khaL \"Jorah mused. \"You must be giving honor in the name of the khaleesi . . .\" \n",
            "Arya glanced at her younger sister. \"He says it is a badge of honor .\" She looked at Myrcella on her \n",
            "steps. \"He says it is a badge of honor to wear it, my daughter. \n",
            "Do you mean that he must be named Joffrey or Joffrey? He must be known as the khaleesi or He is a \n",
            "Goddess. He must be known . . . he must be known. He must be known. \" My son is no khaleesi.\" \n",
            "Mesmerizing him, Arya took away her dagger and pulled it aside. \"He is a black woman with black hair, \n",
            "and a black cloak. I am Khal Drogo's Hand. The black is my shield and the cloak of honor of the \n",
            "world. It was my daughter's cloak, she kept it.\" She looked at the dagger in her hands. \"I will not be giving \n",
            "me honor.\" \n",
            "Her handmaids were not the only ones she'd taken away in that hurry. \"Your daughter is a sigil of honor,\" one of the \n",
            "knights of the royal guard \n",
            "called out. \"She is my shield. I am the Hand that protected her in the womb and that woman will \n",
            "not \n",
            "show me blood on my breast. I do not carry a sigil of honor, a sigil of the khas. Honor is a badge \n",
            "of birth and a badge to the khal. A badge in honor of those who protect her will only bring shame to me. \n",
            "\"Honor's badge, my khal. \" His words made the khal sound solemn. For a long moment, \n",
            "Jorah and his eyes were lost and his breathing shallow. Finally, he said, \"Jorah, stay with me, the \n",
            "Lannisters are fighting. \" Jorah said nothing more. His handmaids hurried over to him. \"You must go \n",
            "to the khal now. The khal is not my father. He is my shield and I will not give me honor. \" \n",
            "Jorah was holding her by the shoulders and tugging at her hair with both of his hands. His handmaids followed the \n",
            "khal on his back, and \n",
            "Arya had taught her to play guard . . . except, in the end, it seemed to them that she was a sigil \n",
            "of honor to them . . . so Arya had decided that she would not look like a sigil of honor. Instead \n",
            "she stood and bowed, knowing her vows . . . and Jorah. He was a sigil, a symbol of honor, \n",
            "no less. \n",
            "The sound of steel echoing from the khal hall echoed through the city, and Arya was frightened. Her handsmaids \n",
            "opened their arms together as the khal came rushing out. \"The khal!\" he cried, \"your honor.\" His footsteps came clackily \n",
            "along the city streets, sharp as sharp teeth, and rang over the Seven Kingdoms like thunder. \n",
            "Arya looked up at Jorah. \"He has come,\" she said. \"His name is the Khal I have come to swear fealty to. \n",
            "Page 636\n",
            "\n",
            "\"There may be days where my life is a struggle, but the days are as a man fights, shall we not \n",
            "begrudge each other?\" \n",
            "Jorah's handmaids helped him take his hand. He looked like the khal now. \"Khaleesi,\" they said. He \n",
            "looked like a khaleesi. He looked like a khal now. The day of Jorah was coming. He had \n",
            "been born the same day and fought for eight years. It was the day when Khal Drogo came from the \n",
            "stars. \n",
            "Arya's sister\n",
            "\n",
            "[201 | 472.70] loss=2.65 avg=2.74\n",
            "[202 | 474.92] loss=2.55 avg=2.74\n",
            "[203 | 477.14] loss=2.30 avg=2.73\n",
            "[204 | 479.36] loss=2.61 avg=2.73\n",
            "[205 | 481.59] loss=2.46 avg=2.73\n",
            "[206 | 483.81] loss=2.49 avg=2.73\n",
            "[207 | 486.03] loss=2.64 avg=2.73\n",
            "[208 | 488.26] loss=2.66 avg=2.73\n",
            "[209 | 490.48] loss=2.53 avg=2.72\n",
            "[210 | 492.72] loss=2.73 avg=2.72\n",
            "[211 | 494.94] loss=2.66 avg=2.72\n",
            "[212 | 497.17] loss=2.59 avg=2.72\n",
            "[213 | 499.39] loss=2.42 avg=2.72\n",
            "[214 | 501.62] loss=2.50 avg=2.72\n",
            "[215 | 503.84] loss=2.57 avg=2.71\n",
            "[216 | 506.08] loss=2.70 avg=2.71\n",
            "[217 | 508.30] loss=2.59 avg=2.71\n",
            "[218 | 510.53] loss=2.60 avg=2.71\n",
            "[219 | 512.75] loss=2.58 avg=2.71\n",
            "[220 | 514.97] loss=2.44 avg=2.71\n",
            "[221 | 517.21] loss=2.62 avg=2.71\n",
            "[222 | 519.44] loss=2.56 avg=2.70\n",
            "[223 | 521.66] loss=2.74 avg=2.70\n",
            "[224 | 523.89] loss=2.69 avg=2.70\n",
            "[225 | 526.11] loss=2.67 avg=2.70\n",
            "[226 | 528.34] loss=2.41 avg=2.70\n",
            "[227 | 530.57] loss=2.37 avg=2.70\n",
            "[228 | 532.79] loss=2.54 avg=2.69\n",
            "[229 | 535.02] loss=2.51 avg=2.69\n",
            "[230 | 537.24] loss=2.54 avg=2.69\n",
            "[231 | 539.46] loss=2.51 avg=2.69\n",
            "[232 | 541.69] loss=2.54 avg=2.69\n",
            "[233 | 543.92] loss=2.47 avg=2.69\n",
            "[234 | 546.14] loss=2.38 avg=2.68\n",
            "[235 | 548.36] loss=2.38 avg=2.68\n",
            "[236 | 550.58] loss=2.59 avg=2.68\n",
            "[237 | 552.81] loss=2.43 avg=2.67\n",
            "[238 | 555.03] loss=2.49 avg=2.67\n",
            "[239 | 557.25] loss=2.27 avg=2.67\n",
            "[240 | 559.47] loss=2.52 avg=2.67\n",
            "[241 | 561.69] loss=2.53 avg=2.67\n",
            "[242 | 563.91] loss=2.40 avg=2.66\n",
            "[243 | 566.14] loss=2.55 avg=2.66\n",
            "[244 | 568.37] loss=2.58 avg=2.66\n",
            "[245 | 570.59] loss=2.38 avg=2.66\n",
            "[246 | 572.81] loss=2.45 avg=2.65\n",
            "[247 | 575.04] loss=2.29 avg=2.65\n",
            "[248 | 577.26] loss=2.40 avg=2.65\n",
            "[249 | 579.48] loss=2.47 avg=2.65\n",
            "[250 | 581.70] loss=2.29 avg=2.64\n",
            "[251 | 583.91] loss=2.53 avg=2.64\n",
            "[252 | 586.13] loss=2.47 avg=2.64\n",
            "[253 | 588.35] loss=2.40 avg=2.64\n",
            "[254 | 590.57] loss=2.59 avg=2.64\n",
            "[255 | 592.80] loss=2.38 avg=2.63\n",
            "[256 | 595.02] loss=2.62 avg=2.63\n",
            "[257 | 597.24] loss=2.42 avg=2.63\n",
            "[258 | 599.47] loss=2.22 avg=2.63\n",
            "[259 | 601.69] loss=2.39 avg=2.62\n",
            "[260 | 603.92] loss=2.49 avg=2.62\n",
            "[261 | 606.15] loss=2.32 avg=2.62\n",
            "[262 | 608.37] loss=2.36 avg=2.62\n",
            "[263 | 610.58] loss=2.45 avg=2.61\n",
            "[264 | 612.80] loss=2.38 avg=2.61\n",
            "[265 | 615.03] loss=2.51 avg=2.61\n",
            "[266 | 617.26] loss=2.47 avg=2.61\n",
            "[267 | 619.48] loss=2.41 avg=2.61\n",
            "[268 | 621.71] loss=2.11 avg=2.60\n",
            "[269 | 623.93] loss=2.55 avg=2.60\n",
            "[270 | 626.16] loss=2.47 avg=2.60\n",
            "[271 | 628.39] loss=2.30 avg=2.60\n",
            "[272 | 630.62] loss=2.43 avg=2.60\n",
            "[273 | 632.85] loss=2.23 avg=2.59\n",
            "[274 | 635.08] loss=2.25 avg=2.59\n",
            "[275 | 637.31] loss=2.09 avg=2.58\n",
            "[276 | 639.54] loss=2.57 avg=2.58\n",
            "[277 | 641.77] loss=2.46 avg=2.58\n",
            "[278 | 644.00] loss=2.55 avg=2.58\n",
            "[279 | 646.22] loss=2.36 avg=2.58\n",
            "[280 | 648.45] loss=2.40 avg=2.58\n",
            "[281 | 650.68] loss=2.51 avg=2.58\n",
            "[282 | 652.91] loss=2.22 avg=2.57\n",
            "[283 | 655.13] loss=2.26 avg=2.57\n",
            "[284 | 657.35] loss=2.38 avg=2.57\n",
            "[285 | 659.58] loss=2.44 avg=2.57\n",
            "[286 | 661.79] loss=2.45 avg=2.56\n",
            "[287 | 664.02] loss=2.45 avg=2.56\n",
            "[288 | 666.25] loss=2.32 avg=2.56\n",
            "[289 | 668.47] loss=2.60 avg=2.56\n",
            "[290 | 670.69] loss=2.29 avg=2.56\n",
            "[291 | 672.91] loss=2.25 avg=2.55\n",
            "[292 | 675.13] loss=2.30 avg=2.55\n",
            "[293 | 677.36] loss=2.34 avg=2.55\n",
            "[294 | 679.58] loss=2.32 avg=2.55\n",
            "[295 | 681.80] loss=2.31 avg=2.54\n",
            "[296 | 684.02] loss=2.22 avg=2.54\n",
            "[297 | 686.24] loss=2.22 avg=2.54\n",
            "[298 | 688.46] loss=2.27 avg=2.53\n",
            "[299 | 690.69] loss=2.36 avg=2.53\n",
            "[300 | 692.90] loss=2.43 avg=2.53\n",
            "======== SAMPLE 1 ========\n",
            ".\"\n",
            "\"The girl has me too\" Jon cried out fiercely. \"And if she's lying about being a little girl, she was only twelve before she \n",
            "was murdered.\" \n",
            "When Sam stood there unmoving, he saw that he too was frightened. \"No,\" he cried out, pulling his dagger into \n",
            "his fist, \"I will not . . . I don't want to . . .I . .I won't\" Against Robb's shouts, the Hound roared \n",
            "out again, and Robb stood dead, clutching the blade with his bloody hand . . . as if the world were turned \n",
            "into a cage, so the world could no longer breathe. \n",
            "\"Your brother killed my brother and mine, \" Jon was dying miserably. He stood up and swung \n",
            "the dagger back in his hand, and the Hound cried out in surprise. Jon was dead, they had killed his \n",
            "brother and mine, they had taken the dead boy . . . Robb raised his head and looked at the \n",
            "blood on the blade. \"You stupid damned bastard. I want to know what Lord Jon has to do. Is he going to \n",
            "behead me? No? What if? Who will pay for his blood? Well, the Lannisters.\" He rose to his feet, with the \n",
            "direwolf in his hand and a long longsword in his fist, and he looked up at Jon, his face a bloody blur. \n",
            "\"No,\" \n",
            "Jon took a step back, looking up at the red eyes that were watching him, \"or kill him\" \n",
            ". \"No\" he repeated stubbornly. \n",
            "\"I will kill you, Jon. The man will behead him and his child. I will chop off his hand, and the world \n",
            "will lie around them like a cage. I will rip down his clothes and his skin and choke him to death. I \n",
            "will slit his neck and tear his throat open with boiling hot needles as they will . . .\" \n",
            "\"I swear to you, if they kill you, I will chop off your throat and feed it to the Others for a long time, and \n",
            "some day you will be all the same. You will be all the same, and it will be a hard thing to come by, \n",
            "but I swear that you will make the hardest hard hard and grow taller and stronger. If I lose you, who else will \n",
            "send you?\" \n",
            "\"The queen,\" Mormont said in a low voice, \"but I will not forsake you.\" \n",
            "Jon lifted his voice and the words made him feel weak again. He could feel his skin grow cold. When the \n",
            "spearheaded lord had gone to rest on the Iron Throne, Jon raised his head. The black stone lay on his \n",
            "sword, staring straight down from above. \"If they kill me\" he shrieked, and the words were cut out, \"kill me, kill me ... \n",
            "kill my brother. Your butcher, the Stark butcher, the Stark Hand.\" \n",
            "The Hound thrust his dagger at him with its long pointed edge. The blow seemed to bring out the \n",
            "spearheaded man's throat and make him bleed more fiercely, but he would not hold his tongue out lest the Hound bite it \n",
            "heart. After that he had nothing but the blood of the stag in his hand. \n",
            "Page 192\n",
            "\n",
            "The Hound would not hear his commands. He lay on his back on the ground and waited for a voice. It had \n",
            "come from the Eyrie of Highgarden, and it was Snow. Jon remembered the great wooden door \n",
            "that had barred the way for the Hound's dagger, the white metal of the keep. The door had been broken in the \n",
            "Night's Watch, and the men on the walls had waited a long time to face the King's Hand. They \n",
            "were no more than rangers, dressed only in grey cloaks and doublets, their belts hung low with the black of the \n",
            "Night's Watch. In their midst the men sat waiting, and waiting, until the door closed again and the \n",
            "Hound came charging on the Tyroshi. \n",
            "Jon knew that if the White Bull did not kill their king then he was no more than a ranger, and he urged \n",
            "his brother to the Wall. The Hound backed away from the man, but the man kept coming, and Jon watched \n",
            "as he moved in the path of the man who had cut his throat. The man who cut his throat? Snow, the Hound \n",
            "Page 193\n",
            "\n",
            "called on the black mary's son who was hunting him. \"Snow, \" Jon shrieked. \n",
            "\"We will kill your king!\" \n",
            "The Hound roared with fury. \n",
            "\"My brother will kill mine!\" The white horseman charged, the red man doubled over in pain. \n",
            "\n",
            "\n",
            "[301 | 706.01] loss=2.41 avg=2.53\n",
            "[302 | 708.23] loss=2.39 avg=2.53\n",
            "[303 | 710.45] loss=2.47 avg=2.53\n",
            "[304 | 712.67] loss=2.63 avg=2.53\n",
            "[305 | 714.89] loss=2.20 avg=2.53\n",
            "[306 | 717.11] loss=2.18 avg=2.52\n",
            "[307 | 719.33] loss=2.11 avg=2.52\n",
            "[308 | 721.55] loss=2.31 avg=2.52\n",
            "[309 | 723.77] loss=2.18 avg=2.51\n",
            "[310 | 725.99] loss=2.48 avg=2.51\n",
            "[311 | 728.21] loss=2.34 avg=2.51\n",
            "[312 | 730.43] loss=2.01 avg=2.51\n",
            "[313 | 732.66] loss=2.45 avg=2.50\n",
            "[314 | 734.88] loss=2.21 avg=2.50\n",
            "[315 | 737.10] loss=2.21 avg=2.50\n",
            "[316 | 739.33] loss=2.41 avg=2.50\n",
            "[317 | 741.55] loss=2.33 avg=2.50\n",
            "[318 | 743.78] loss=2.16 avg=2.49\n",
            "[319 | 746.00] loss=2.35 avg=2.49\n",
            "[320 | 748.22] loss=2.22 avg=2.49\n",
            "[321 | 750.45] loss=2.34 avg=2.49\n",
            "[322 | 752.67] loss=2.12 avg=2.48\n",
            "[323 | 754.90] loss=2.43 avg=2.48\n",
            "[324 | 757.12] loss=2.19 avg=2.48\n",
            "[325 | 759.35] loss=2.15 avg=2.48\n",
            "[326 | 761.58] loss=1.96 avg=2.47\n",
            "[327 | 763.81] loss=2.58 avg=2.47\n",
            "[328 | 766.04] loss=2.07 avg=2.47\n",
            "[329 | 768.26] loss=2.08 avg=2.46\n",
            "[330 | 770.49] loss=2.12 avg=2.46\n",
            "[331 | 772.71] loss=2.41 avg=2.46\n",
            "[332 | 774.94] loss=2.29 avg=2.46\n",
            "[333 | 777.16] loss=2.35 avg=2.46\n",
            "[334 | 779.39] loss=2.10 avg=2.45\n",
            "[335 | 781.61] loss=2.20 avg=2.45\n",
            "[336 | 783.84] loss=2.20 avg=2.45\n",
            "[337 | 786.07] loss=2.12 avg=2.44\n",
            "[338 | 788.30] loss=2.38 avg=2.44\n",
            "[339 | 790.53] loss=2.19 avg=2.44\n",
            "[340 | 792.75] loss=2.10 avg=2.44\n",
            "[341 | 794.98] loss=2.35 avg=2.44\n",
            "[342 | 797.21] loss=2.07 avg=2.43\n",
            "[343 | 799.44] loss=2.00 avg=2.43\n",
            "[344 | 801.66] loss=2.14 avg=2.43\n",
            "[345 | 803.89] loss=2.09 avg=2.42\n",
            "[346 | 806.12] loss=2.01 avg=2.42\n",
            "[347 | 808.34] loss=2.40 avg=2.42\n",
            "[348 | 810.57] loss=2.02 avg=2.41\n",
            "[349 | 812.79] loss=2.28 avg=2.41\n",
            "[350 | 815.01] loss=2.24 avg=2.41\n",
            "[351 | 817.23] loss=2.23 avg=2.41\n",
            "[352 | 819.45] loss=2.41 avg=2.41\n",
            "[353 | 821.68] loss=2.09 avg=2.41\n",
            "[354 | 823.91] loss=2.33 avg=2.40\n",
            "[355 | 826.13] loss=2.09 avg=2.40\n",
            "[356 | 828.35] loss=2.11 avg=2.40\n",
            "[357 | 830.57] loss=2.39 avg=2.40\n",
            "[358 | 832.79] loss=2.03 avg=2.39\n",
            "[359 | 835.02] loss=1.96 avg=2.39\n",
            "[360 | 837.24] loss=1.95 avg=2.39\n",
            "[361 | 839.46] loss=1.79 avg=2.38\n",
            "[362 | 841.69] loss=2.10 avg=2.38\n",
            "[363 | 843.91] loss=2.34 avg=2.38\n",
            "[364 | 846.14] loss=1.82 avg=2.37\n",
            "[365 | 848.36] loss=2.11 avg=2.37\n",
            "[366 | 850.59] loss=2.14 avg=2.36\n",
            "[367 | 852.81] loss=2.17 avg=2.36\n",
            "[368 | 855.03] loss=2.26 avg=2.36\n",
            "[369 | 857.26] loss=2.08 avg=2.36\n",
            "[370 | 859.49] loss=1.87 avg=2.35\n",
            "[371 | 861.71] loss=2.26 avg=2.35\n",
            "[372 | 863.93] loss=2.12 avg=2.35\n",
            "[373 | 866.15] loss=2.07 avg=2.35\n",
            "[374 | 868.37] loss=2.22 avg=2.35\n",
            "[375 | 870.60] loss=2.37 avg=2.35\n",
            "[376 | 872.84] loss=2.45 avg=2.35\n",
            "[377 | 875.06] loss=2.01 avg=2.34\n",
            "[378 | 877.27] loss=2.12 avg=2.34\n",
            "[379 | 879.50] loss=2.14 avg=2.34\n",
            "[380 | 881.72] loss=2.09 avg=2.34\n",
            "[381 | 883.95] loss=2.05 avg=2.33\n",
            "[382 | 886.18] loss=1.97 avg=2.33\n",
            "[383 | 888.40] loss=1.92 avg=2.33\n",
            "[384 | 890.63] loss=2.23 avg=2.33\n",
            "[385 | 892.85] loss=1.92 avg=2.32\n",
            "[386 | 895.08] loss=1.91 avg=2.32\n",
            "[387 | 897.31] loss=1.82 avg=2.31\n",
            "[388 | 899.53] loss=2.26 avg=2.31\n",
            "[389 | 901.75] loss=2.41 avg=2.31\n",
            "[390 | 903.98] loss=2.09 avg=2.31\n",
            "[391 | 906.21] loss=2.14 avg=2.31\n",
            "[392 | 908.44] loss=2.08 avg=2.31\n",
            "[393 | 910.66] loss=2.29 avg=2.31\n",
            "[394 | 912.88] loss=2.36 avg=2.31\n",
            "[395 | 915.11] loss=2.12 avg=2.30\n",
            "[396 | 917.34] loss=2.00 avg=2.30\n",
            "[397 | 919.58] loss=2.08 avg=2.30\n",
            "[398 | 921.80] loss=2.26 avg=2.30\n",
            "[399 | 924.02] loss=2.07 avg=2.30\n",
            "[400 | 926.25] loss=2.01 avg=2.29\n",
            "======== SAMPLE 1 ========\n",
            "CaRpIy1JyAoQFy1AQR \n",
            "Arya stepped over the crossbow and aimed her bow at Ser Jorah, who fired back, but his fire ripped through the fabric and struck \n",
            "Arya in the ribs. She shattered her ribs. \n",
            "And as the Lord of the Houses of the Andals and the Hand of the King moved closer, the Lord of the \n",
            "Seven Kingdoms looked down and saw that Jhogo was dead. \n",
            "The King's Door opened. Lord Eddard Stark rode forth from the Great Hall to the dais; he did not look \n",
            "Page 556\n",
            "\n",
            "cravenically after he had taken hold of Jorah and his horses. \n",
            "He had been anointed with the seven oils of the seven firepillars, a sign that his fire had come from the \n",
            "halc outside the Wall, and that fire was no fire. \n",
            "\"My lord Hand,\" Jhogo said with a long downward stroke of his hand, \"if true, the Eyrie looks \n",
            "Page 557\n",
            "\n",
            "like a room of the heart. The last thing I wanted was for someone so young to rule like my king.\" He bowed \n",
            "faintly, and the three High Stewards gathered round to offer their soft kisses to the knight around Rhaegar's \n",
            "sword. \n",
            "\"You are not of the Nine,\" the knight said at last and bowed his head. \"I would have loved to sit and bow my head and \n",
            "be a knight.\" \n",
            "\"As you say,\" Arya said with a certain jesting of sadness on her face, though she still took great \n",
            "gracious delight in that. Cersei Lannister had been waiting in the south since the day her uncle took the \n",
            "knight south after Drogo, and yet she had not been here. She had not felt as much as Prince Rhaegar, or even \n",
            "Jaime, or Rhaego. She had not seen the fire, not even once. The fire had haunted her almost \n",
            "every waking moment since the day Drogo returned, as blood and death poured forth and began to mingle among the \n",
            "black stone of the walls. \n",
            "There were no whispers of Rhaego or the others. The only sound was the rustle of horses' \n",
            "feet and the soft clang of war drums. The night sky was red and sickly, so full were the stars that \n",
            "almost even the gods were lost within the empty windows of their apartments. When dusk came, the moon turned its \n",
            "helmet to the sky, and the stars burned in the depths of the night. Yet even in the dead of night, \n",
            "stars did not see. This was the realm so many had come to see, the realm beyond the walls of death. This \n",
            "was the realm the gods had known before long, and the gods who had lived before it. She had lived \n",
            "since the dawn of the \n",
            "Seven Kingdoms. \n",
            "Page 558\n",
            "\n",
            "\"My lady Hand,\" Rhaego said suddenly, \n",
            "\"the Eyrie offers wonders.\" He bowed to her. \"Beyond the Wall, the stars wink out, and in the night \n",
            "the world sleeps. So it seemed to me that if the gods heard the words of my people, that they might win, \n",
            "and that they might die for us. I do not comprehend that.\" \n",
            "\"So it did,\" Arya said. Her hand was trembling strangely. \"I don't know this place, do I? You know the \n",
            "story?\" \n",
            "\"So. I do. And if you do, surely you know as well. Tell it, and speak up. I have a thousand gods to ask \n",
            "to speak for me, the very thing I wished to deny you. I need counsel, help, and a good king and \n",
            "fellowcastle lord to help me. Gods and men must be coexiled.\" \n",
            "Page 559\n",
            "\n",
            "\"Is there a way to kill Eddard?\" Lord Mormont asked, his grey, round features aching from the long, cold \n",
            "campaign. \n",
            "\"The Eyrie is a fortress of the very gods,\" Arya told him. \"It is not for the faint-hearted to enter, not here. When Lord \n",
            "Eddard Stark came across our queen, Lord Karstark, she begged me for food and her help, so I found her \n",
            "mocked and beaten, in the cold of winter. I have heard you hold court with Lady Stark in your many a \n",
            "hall and lavish parties go with it.\" \n",
            "\"Sweet words, indeed,\" Cersei Lannister said. \"I know King Robert. He will not trouble Lord Eddard or his \n",
            "daughter, I trust. Nor will he .\n",
            "\n",
            "[401 | 939.29] loss=1.84 avg=2.29\n",
            "[402 | 941.51] loss=2.16 avg=2.29\n",
            "[403 | 943.73] loss=1.76 avg=2.28\n",
            "[404 | 945.96] loss=1.76 avg=2.28\n",
            "[405 | 948.18] loss=2.21 avg=2.28\n",
            "[406 | 950.40] loss=2.20 avg=2.28\n",
            "[407 | 952.62] loss=2.23 avg=2.28\n",
            "[408 | 954.85] loss=1.94 avg=2.27\n",
            "[409 | 957.07] loss=2.18 avg=2.27\n",
            "[410 | 959.30] loss=1.79 avg=2.27\n",
            "[411 | 961.52] loss=1.66 avg=2.26\n",
            "[412 | 963.74] loss=1.88 avg=2.26\n",
            "[413 | 965.96] loss=2.03 avg=2.25\n",
            "[414 | 968.19] loss=1.66 avg=2.25\n",
            "[415 | 970.41] loss=2.36 avg=2.25\n",
            "[416 | 972.64] loss=2.14 avg=2.25\n",
            "[417 | 974.86] loss=1.95 avg=2.24\n",
            "[418 | 977.08] loss=1.88 avg=2.24\n",
            "[419 | 979.30] loss=1.78 avg=2.24\n",
            "[420 | 981.53] loss=1.96 avg=2.23\n",
            "[421 | 983.75] loss=1.98 avg=2.23\n",
            "[422 | 985.98] loss=1.98 avg=2.23\n",
            "[423 | 988.20] loss=1.64 avg=2.22\n",
            "[424 | 990.41] loss=2.04 avg=2.22\n",
            "[425 | 992.64] loss=2.02 avg=2.22\n",
            "[426 | 994.87] loss=2.09 avg=2.22\n",
            "[427 | 997.09] loss=1.90 avg=2.21\n",
            "[428 | 999.31] loss=2.05 avg=2.21\n",
            "[429 | 1001.53] loss=1.77 avg=2.21\n",
            "[430 | 1003.75] loss=2.17 avg=2.21\n",
            "[431 | 1005.98] loss=2.19 avg=2.21\n",
            "[432 | 1008.20] loss=2.02 avg=2.21\n",
            "[433 | 1010.42] loss=2.04 avg=2.20\n",
            "[434 | 1012.64] loss=1.96 avg=2.20\n",
            "[435 | 1014.86] loss=2.01 avg=2.20\n",
            "[436 | 1017.08] loss=2.09 avg=2.20\n",
            "[437 | 1019.30] loss=1.90 avg=2.20\n",
            "[438 | 1021.52] loss=1.83 avg=2.19\n",
            "[439 | 1023.74] loss=2.05 avg=2.19\n",
            "[440 | 1025.96] loss=1.98 avg=2.19\n",
            "[441 | 1028.18] loss=1.74 avg=2.18\n",
            "[442 | 1030.40] loss=1.78 avg=2.18\n",
            "[443 | 1032.63] loss=1.83 avg=2.18\n",
            "[444 | 1034.84] loss=2.07 avg=2.17\n",
            "[445 | 1037.07] loss=1.94 avg=2.17\n",
            "[446 | 1039.29] loss=1.91 avg=2.17\n",
            "[447 | 1041.51] loss=1.99 avg=2.17\n",
            "[448 | 1043.73] loss=1.89 avg=2.16\n",
            "[449 | 1045.95] loss=1.69 avg=2.16\n",
            "[450 | 1048.17] loss=1.69 avg=2.16\n",
            "[451 | 1050.38] loss=1.40 avg=2.15\n",
            "[452 | 1052.60] loss=1.68 avg=2.14\n",
            "[453 | 1054.83] loss=2.15 avg=2.14\n",
            "[454 | 1057.05] loss=2.02 avg=2.14\n",
            "[455 | 1059.27] loss=2.01 avg=2.14\n",
            "[456 | 1061.48] loss=2.03 avg=2.14\n",
            "[457 | 1063.70] loss=1.85 avg=2.14\n",
            "[458 | 1065.93] loss=1.91 avg=2.13\n",
            "[459 | 1068.15] loss=1.89 avg=2.13\n",
            "[460 | 1070.36] loss=1.99 avg=2.13\n",
            "[461 | 1072.58] loss=1.90 avg=2.13\n",
            "[462 | 1074.81] loss=1.91 avg=2.13\n",
            "[463 | 1077.03] loss=1.84 avg=2.12\n",
            "[464 | 1079.25] loss=1.91 avg=2.12\n",
            "[465 | 1081.47] loss=1.92 avg=2.12\n",
            "[466 | 1083.69] loss=1.94 avg=2.12\n",
            "[467 | 1085.91] loss=1.71 avg=2.11\n",
            "[468 | 1088.13] loss=1.99 avg=2.11\n",
            "[469 | 1090.36] loss=1.99 avg=2.11\n",
            "[470 | 1092.59] loss=1.66 avg=2.11\n",
            "[471 | 1094.81] loss=1.93 avg=2.10\n",
            "[472 | 1097.03] loss=1.62 avg=2.10\n",
            "[473 | 1099.25] loss=2.15 avg=2.10\n",
            "[474 | 1101.48] loss=2.10 avg=2.10\n",
            "[475 | 1103.70] loss=1.88 avg=2.10\n",
            "[476 | 1105.92] loss=1.79 avg=2.09\n",
            "[477 | 1108.14] loss=1.86 avg=2.09\n",
            "[478 | 1110.35] loss=2.08 avg=2.09\n",
            "[479 | 1112.58] loss=2.12 avg=2.09\n",
            "[480 | 1114.81] loss=2.10 avg=2.09\n",
            "[481 | 1117.03] loss=1.87 avg=2.09\n",
            "[482 | 1119.25] loss=1.77 avg=2.09\n",
            "[483 | 1121.46] loss=1.93 avg=2.09\n",
            "[484 | 1123.68] loss=1.71 avg=2.08\n",
            "[485 | 1125.91] loss=1.69 avg=2.08\n",
            "[486 | 1128.13] loss=2.02 avg=2.08\n",
            "[487 | 1130.34] loss=1.89 avg=2.07\n",
            "[488 | 1132.56] loss=1.60 avg=2.07\n",
            "[489 | 1134.78] loss=2.03 avg=2.07\n",
            "[490 | 1137.01] loss=1.83 avg=2.07\n",
            "[491 | 1139.23] loss=1.82 avg=2.06\n",
            "[492 | 1141.46] loss=1.90 avg=2.06\n",
            "[493 | 1143.68] loss=2.05 avg=2.06\n",
            "[494 | 1145.90] loss=1.80 avg=2.06\n",
            "[495 | 1148.12] loss=1.49 avg=2.05\n",
            "[496 | 1150.34] loss=1.89 avg=2.05\n",
            "[497 | 1152.58] loss=1.80 avg=2.05\n",
            "[498 | 1154.80] loss=1.92 avg=2.05\n",
            "[499 | 1157.02] loss=1.54 avg=2.04\n",
            "[500 | 1159.24] loss=1.90 avg=2.04\n",
            "Saving checkpoint/run1/model-500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f2d47a-832e-44fb-da51-2ce17bff6689"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? Are the Arryns all alike?\" \n",
            "\"The same,\" Tyrion replied, his eyes searching the crowd. \"The one with the black brothers is the \n",
            "true one. The other one is the false one.\" \n",
            "Viserys looked apprehensive. \"I do not trust the gods,\" he said irritably. \"I have not spoken to the \n",
            "Lannisters.\" \n",
            "Page 68\n",
            "\n",
            "\"They are all one, my lord.\" Tyrion was beginning to understand. \"Why should they be two?\" \n",
            "\"The gods are kind,\" Viserys said. \"The oldest are gods of the great Houses, and the last to leave \n",
            "their \n",
            "houses. The gods do not leave\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}